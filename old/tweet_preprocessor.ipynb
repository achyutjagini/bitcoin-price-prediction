{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":39791,"status":"ok","timestamp":1661349891259,"user":{"displayName":"Achyut Jagini","userId":"03826907384707487236"},"user_tz":-330},"id":"TQ_3q8RX0rbx","outputId":"2f4ae6b4-38c6-41d9-8e4c-e1028330f980"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: vaderSentiment in /usr/local/lib/python3.7/dist-packages (3.3.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from vaderSentiment) (2.23.0)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->vaderSentiment) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->vaderSentiment) (2022.6.15)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->vaderSentiment) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->vaderSentiment) (2.10)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting langdetect\n","  Downloading langdetect-1.0.9.tar.gz (981 kB)\n","\u001b[K     |████████████████████████████████| 981 kB 6.4 MB/s \n","\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from langdetect) (1.15.0)\n","Building wheels for collected packages: langdetect\n","  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993242 sha256=d2c0b63f2888558ba477dacf9b172feecd0b73275f697291483c119f14a2b737\n","  Stored in directory: /root/.cache/pip/wheels/c5/96/8a/f90c59ed25d75e50a8c10a1b1c2d4c402e4dacfa87f3aff36a\n","Successfully built langdetect\n","Installing collected packages: langdetect\n","Successfully installed langdetect-1.0.9\n","Mounted at /content/gdrive\n"]}],"source":["!pip install vaderSentiment\n","from google.colab import drive\n","from time import sleep\n","import json\n","import pandas as pd\n","import io\n","import re\n","import numpy as np\n","from tqdm import tqdm\n","import pandas as pd\n","from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n","from tqdm import tnrange, tqdm_notebook, tqdm\n","import pandas as pd\n","from sklearn import preprocessing\n","import matplotlib.pyplot as plt\n","drive.mount(\"/content/gdrive\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3630,"status":"ok","timestamp":1661349894880,"user":{"displayName":"Achyut Jagini","userId":"03826907384707487236"},"user_tz":-330},"id":"QUwCU4UisefM","outputId":"e7a1ff8c-ca2e-434c-8f9f-b42688804b3e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.7)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from nltk) (4.64.0)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk) (1.1.0)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.7/dist-packages (from nltk) (2022.6.2)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk) (7.1.2)\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n","[nltk_data]   Package omw-1.4 is already up-to-date!\n"]}],"source":["! pip install nltk\n","import nltk\n","from nltk.stem.wordnet import WordNetLemmatizer\n","\n","nltk.download('wordnet')\n","nltk.download('stopwords')\n","nltk.download('punkt')\n","stop_words = nltk.corpus.stopwords.words(['english'])\n","nltk.download('omw-1.4')\n","from nltk.tokenize import TweetTokenizer\n","from nltk.stem.wordnet import WordNetLemmatizer\n","lem = WordNetLemmatizer()"]},{"cell_type":"code","source":[],"metadata":{"id":"aSY97jELJypV"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"XGSm_lA008nV","outputId":"fbe6a29c-981c-4e20-c5a9-9e17980e4438"},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 2830476/2830476 [29:37<00:00, 1592.49it/s]\n"]}],"source":["RAW_TWEETS_FILE = \"tweets_2.csv\"\n","CLEAN_TWEETS_FILE = \"CLEAN_TWEETS_FILE.csv\"\n","\n","\n","def get_raw_tweets():\n","  return pd.read_csv(f'/content/gdrive/My Drive/capstone project/datasets/tweets/{RAW_TWEETS_FILE}', low_memory=False)\n","\n","\n","def clean_tweets(df):\n","  # enumerating through all the tweets in the dataset and updating them\n","  for i, s in enumerate(tqdm(df['text'], position=0, leave=True)):\n","      text = str(df.loc[i, 'text'])\n","      text = text.lower() # converting to lowercaseF\n","      \n","      text = text.replace(\"#\", \"\") # remove hashtags\n","      text = re.sub('https?://(?:[-\\w.]|(?:%[\\da-fA-F]{2}))+', '', text, flags=re.MULTILINE) # removes hyperlinks\n","      text = re.sub('@\\\\w+ *', '', text, flags=re.MULTILINE) # removes mentions\n","      text = re.sub('[^A-Za-z]+', ' ', text) # Remove characters that are not in the English alphabet\n","      text = TweetTokenizer().tokenize(text) # Tokenize\n","\n","      # tokens_without_punc = [w for w in text if w.isalpha()] # Remove non-alphabetic characters\n","      tokens_without_sw = [t for t in text if t not in stop_words] # Removing Stopwords\n","      text_cleaned = [lem.lemmatize(t) for t in tokens_without_sw] # lemmatization\n","\n","      df.loc[i, 'text'] = \" \".join(text_cleaned)\n","\n","\n","def convert_dataframe_to_csv(df, filename):\n","  df.to_csv(f'/content/gdrive/My Drive/capstone project/modified datasets/{filename}', header=True, encoding='utf-8',index=False) # convert and save as csv\n","\n","\n","if __name__ == \"__main__\":\n","  raw_tweets_df = get_raw_tweets()\n","  clean_tweets(raw_tweets_df)\n","  convert_dataframe_to_csv(raw_tweets_df, CLEAN_TWEETS_FILE)\n","\n","\n"]},{"cell_type":"code","source":[],"metadata":{"id":"mOPJ0itKqwZR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","\n"],"metadata":{"id":"gB20--FRqsm4"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"collapsed_sections":[],"name":"tweet_preprocessor.py","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}